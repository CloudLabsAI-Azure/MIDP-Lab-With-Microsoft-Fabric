# Exercise 3: Ingest data into the lakehouse

##### **Estimated Duration: 60 minutes**

In this exercise, you will ingest additional dimensional and fact tables from the Wide World Importers (WWI) dataset into the lakehouse. You will configure a data pipeline within Microsoft Fabric to enable efficient data ingestion. The data will be validated and organized into a structured format, ensuring it is ready for further analysis and reporting. This exercise demonstrates the platform's ability to handle complex data processes seamlessly, ensuring data is prepared for advanced analysis and insights.

## Lab objective

- Task 1: Ingest data

## Task 1: Ingest data

**Objective:** Import the Wide World Importers dataset into the lakehouse by setting up a data pipeline.

1. Open/Navigate back to the **Fabric Workspace**, click on the **+ New item (1)** button and select **Data pipeline (2)**.

   ![](../media/221124(6).png)

2. In the **New pipeline** dialog box, specify the name as **IngestDataFromSourceToLakehouse (1)** and select **Create (2)**. A new data factory pipeline is created and opened.

   ![new-data-pipeline](../media/10/02.png)

3. On your newly created data factory pipeline, select **Copy data assistant** to add an activity to the pipeline.

   ![add-pipeline-copy-data](../media/10/03at.png)

4. Next, set up an HTTP connection to import the sample World Wide Importers data into the Lakehouse. From the list of Choose data sources, search for **Http (1)** and select **Http (2)**.

   ![add-pipeline-copy-data](../media/10/03ab.png)

5. In the **Connect to data source** window, enter the details from the table below and select **Next**.

    | Property | 	Value |
    | -- | -- |
    | **URL** | `https://assetsprod.microsoft.com/en-us/wwi-sample-dataset.zip` |
    | **Connection** | Create a new connection |
    | **Connection name** | wwisampledata |
    | **Data gateway** | None |
    | **Authentication kind** | Anonymous |

   ![](../media/221124(7).png)

5. In the next step, enable the **Binary copy** and choose **ZipDeflate (.zip)** as the **Compression type** since the source is a .zip file. Keep the other fields at their default values and click **Next**.

   ![add-pipeline-copy-data](../media/10/03ad.png)

6. In **Choose data destination**, select your lakehouse as **wwilakehouse**.

   ![add-pipeline-copy-data](../media/10/03ae.png)

7. In the **Connect to data destination** window, specify the **Root folder** as **Files** and click **Next**. This will write the data to the Files section of the lakehouse.

   ![add-pipeline-copy-data](../media/10/03af.png)

8. Choose the **File format** as **Binary** for the destination. Click **Next** and then **Save + Run**. You can schedule pipelines to refresh data periodically. In this tutorial, we only run the pipeline once. The data copy process takes approximately 10-15 minutes to complete.
 
   ![add-pipeline-copy-data](../media/10/03ag.png)

   ![add-pipeline-copy-data](../media/10/03ah.png)

   >**Note:** If you are not able to see **Binary** option then do not select any option, it will take the binary format by default.

9. You can monitor the pipeline execution and activity in the Output tab. You can also view detailed data transfer information by selecting the glasses icon next to the pipeline name, which appears when you hover over the name

   ![add-pipeline-copy-data](../media/10/03ai.png)

    >**Note**: The process will take up to 20 minutes to complete. 

11. Now, navigate back to **Fabric workspace** and select **wwilakehouse** to launch **Lakehouse explorer**.

    ![add-pipeline-copy-data](../media/09/22a.png)

12. Validate that in the Lakehouse Explorer view, a new folder **WideWorldImportersDW** has been created and data for all the tables have been copied there.

    >**Note**: Refresh the Files section from the drop-down. 

    ![add-pipeline-copy-data](../media/10/03ak.png)

13. The data is created under the **Files** section of the lakehouse explorer. A new folder with GUID contains all the needed data. Rename the GUID to **wwi-raw-data**.

    ![add-pipeline-copy-data](../media/10/03al.png)

    ![add-pipeline-copy-data](../media/10/03am.png)

14. At this point, data for all the tables has been successfully copied into the **wwi-raw-data** folder.

    ![add-pipeline-copy-data](../media/221124(8).png)

   > **Congratulations** on completing the task! Now, it's time to validate it. Here are the steps:
   > - If you receive a success message, you can proceed to the next task.
   > - If not, carefully read the error message and retry the step, following the instructions in the lab guide. 
   > - If you need any assistance, please contact us at cloudlabs-support@spektrasystems.com. We are available 24/7 to help you out.
 
   <validation step="97e3f082-96e0-4665-bdab-1a69221a56d9" />

## Review:
In this exercise, you have:

- Created a data pipeline in Fabric Workspace to facilitate data ingestion.

- Imported the WWI dataset into the lakehouse, leveraging the pipeline to handle the transfer from an HTTP source.

- Validated and organized the imported data into a structured format for further analysis.


## Conclusion
Congratulations on successfully completing the lab. Through this comprehensive exercise, you have demonstrated proficiency in leveraging Microsoft Fabric for end-to-end data workflows. Beginning with the creation of a Fabric workspace, you assigned administrative roles and activated a 60-day trial to explore the platform's capabilities. You proceeded to build a lakehouse, effectively managing both structured and unstructured data and utilized Power BI to create insightful visualizations.  

Additionally, you designed and executed a data pipeline to ingest and organize dimensional and fact tables from the Wide World Importers dataset, showcasing the seamless integration of data engineering, analytics, and reporting within Microsoft Fabric. This hands-on experience has equipped you with the practical skills needed to manage, analyze, and visualize data in a unified environment, positioning you to handle complex workflows and deliver data-driven insights in professional scenarios.

### You have successfully completed the lab.
